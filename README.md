# VQA Hub

A modular **hub-and-unit model framework** for **Visual Question Answering (VQA)** research.  
This project explores whether multiple smaller, specialized text and vision models can work together in a back-and-forth process to rival large multimodal models.

---

## ğŸ“Œ Overview
The **Hub** coordinates multiple **unit models**:
- **Text models** (e.g., GPT-2) for reasoning and question understanding
- **Vision models** (e.g., Ollama BLIP captioner) for image description
- **Fusion models** for combining text + vision data

The hub routes tasks between unit models, passing intermediate results until an answer is ready.

---

## ğŸ§  Goal
Evaluate whether a distributed chain of small models can:
- Match or exceed the accuracy of large multimodal models
- Reduce computation costs
- Provide more flexible, interpretable reasoning steps

---

## ğŸ— Architecture
```
Image â†’ Vision Model â†’ Hub â†’ Text Model â†’ Hub â†’ (repeat until answer) â†’ Output
```

**Technologies Used**:
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [LangChain](https://www.langchain.com/)
- [Ollama](https://ollama.com/)
- Python 3.10+

---

## ğŸš€ Installation
```bash
git clone https://github.com/abhayasho/vqa-hub.git
cd vqa-hub
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -e .
```

---

## ğŸ“· Example Usage
```bash
python -m vqa_hub.pipelines.vqa_pipeline   --image ./demo.jpg   --question "What is happening?"   --config configs/default.yaml
```

---

## ğŸ“‚ Project Structure
```
vqa-hub/
â”‚â”€â”€ configs/           # YAML configs for models and pipelines
â”‚â”€â”€ data/              # Sample images
â”‚â”€â”€ src/vqa_hub/       # Source code
â”‚â”€â”€ requirements.txt   # Dependencies
â”‚â”€â”€ README.md          # Project documentation
```

---

## ğŸ§ª Research Context
This framework is part of a research project investigating **hub-and-unit model orchestration** for multimodal tasks, focusing on **VQA** as a benchmark.

---

## ğŸ“œ License
MIT License â€” feel free to use and modify for research or development.
